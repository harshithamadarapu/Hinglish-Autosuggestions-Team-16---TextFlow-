{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+sW3v8ibKxrrf3r3TfdIr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshithamadarapu/Team16_Hinglish-Auto-suggestions/blob/main/tokenize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXR-yw-bIdFQ",
        "outputId": "bfeb5f18-2d9f-47f5-988e-77b5a257f488"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "train_data = pd.read_csv('/content/pre_train.csv')\n",
        "val_data = pd.read_csv('/content/pre_val.csv')\n",
        "\n",
        "#as there are some rows which are just 'Nan' values.\n",
        "# so identify the non-string rows in the 'phrases' column\n",
        "non_string_train = train_data[~train_data['phrases'].apply(lambda x: isinstance(x, str))]\n",
        "non_string_val = val_data[~val_data['phrases'].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "# Printing the non-string rows in both train and val datasets\n",
        "print(\"Non-string rows in train data:\")\n",
        "print(non_string_train)\n",
        "\n",
        "print(\"Non-string rows in val data:\")\n",
        "print(non_string_val)\n",
        "\n",
        "# Removing the non-string rows in both datasets\n",
        "train_data_clean = train_data[train_data['phrases'].apply(lambda x: isinstance(x, str))]\n",
        "val_data_clean = val_data[val_data['phrases'].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "# Tokenize with word_tokenize\n",
        "train_data_clean['tokens'] = train_data_clean['phrases'].apply(lambda x: word_tokenize(x))\n",
        "val_data_clean['tokens'] = val_data_clean['phrases'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "# Saving the cleaned and tokenized data into new csv files\n",
        "train_data_clean.to_csv('/content/token_train.csv', index=False)\n",
        "val_data_clean.to_csv('/content/token_val.csv', index=False)\n",
        "\n",
        "print(\"Tokenized and cleaned data saved to tokenized_train_clean.csv and tokenized_val_clean.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksAByGzZI0pc",
        "outputId": "02fc5855-6a27-4168-a1ea-c8248aac44db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-string rows in train data:\n",
            "       phrases\n",
            "6468       NaN\n",
            "7170       NaN\n",
            "7279       NaN\n",
            "7466       NaN\n",
            "7732       NaN\n",
            "130774     NaN\n",
            "184746     NaN\n",
            "187896     NaN\n",
            "Non-string rows in val data:\n",
            "     phrases\n",
            "1428     NaN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-cc25d8fb9c76>:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data_clean['tokens'] = train_data_clean['phrases'].apply(lambda x: word_tokenize(x))\n",
            "<ipython-input-23-cc25d8fb9c76>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_data_clean['tokens'] = val_data_clean['phrases'].apply(lambda x: word_tokenize(x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized and cleaned data saved to tokenized_train_clean.csv and tokenized_val_clean.csv\n"
          ]
        }
      ]
    }
  ]
}