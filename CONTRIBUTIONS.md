REPORT ON WHAT EACH TEAM MATE DID.

## SE22UARI080:
https://www.kaggle.com/datasets/bhuvanavijaya/nlp-autosuggestion
1.preprocessing the data : using regax removed all the non alphabets.

2.tokenization : tokenized the data using split() because since it is auto suggestion I thought every word is important. But when it comes to training with Bert models I used the modelâ€™s tokenizer and for LSTM I used to normal tokenization.

3.TRAINING AND FINETUNEING THE DATE: eventhough they are failures I experimented with not just these but several models like multilinguial bert too.but the model is too heavy to execute to the end. with indic bert,

4. WORD EMBEDDINGS AND CONTEXT EMBEDDINGS:
Context embeddings are created with LSTM.


