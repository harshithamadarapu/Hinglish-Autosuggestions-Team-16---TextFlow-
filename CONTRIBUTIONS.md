REPORT ON WHAT EACH TEAM MATE DID.

## SE22UARI080
1.preprocessing the data : using regax removed all the non alphabets.
2.tokenization : tokenized the data using split() because since it is auto suggestion I thought every word is important. But when it comes to training with Bert models I used the modelâ€™s tokenizer and for LSTM I used to normal tokenization.
4.TRAINING AND FINETUNEING THE DATE: eventhough they are failures I experimented with not just these but several models like multilinguial bert too.but the model is too heavy to execute to the end.
5. WORD EMBEDDINGS AND CONTEXT EMBEDDINGS:
OneDrive  this links leads to a folder where I uploaded the embeddings.
Context embeddings are created with LSTM.


